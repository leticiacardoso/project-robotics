{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação das bibliotecas mencionadas no readme.md\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBAIXAR https://pjreddie.com/media/files/yolov3.weights !!!\\nArquivo muito grande para o github.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "BAIXAR https://pjreddie.com/media/files/yolov3.weights !!!\n",
    "Arquivo muito grande para o github.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''o modelo YOLOv3 pré-treinado (pesos e arquivos de configuração)\n",
    "https://github.com/pjreddie/darknet/blob/master/data/coco.names -> arquivo contendo a lista de classes, cada linha representa uma classe\n",
    "\n",
    "https://github.com/pjreddie/darknet/blob/master/cfg/yolov3.cfg -> arquivo de configuração do modelo\n",
    "\n",
    "https://pjreddie.com/media/files/yolov3.weights -> arquivo de pesos'''\n",
    "\n",
    "# Carregar as configurações e pesos do modelo YOLOv3 pré-treinado para detecção de objetos\n",
    "net = cv2.dnn.readNet(\"../../yolov3.weights\", \"./arquivos_config/yolov3.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar lista de classes que o modelo YOLOv3 foi treinado para reconhecer (cada linha representa uma classe)\n",
    "classes = []\n",
    "with open(\"./arquivos_config/coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar a imagem para teste\n",
    "image = cv2.imread(\"./imagens/imagem2.png\")\n",
    "image_original = cv2.imread(\"./imagens/imagem2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter as camadas de saída da rede YOLOv3\n",
    "output_layers = net.getUnconnectedOutLayersNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redimensionar a imagem e normalizar os valores dos pixels\n",
    "# (pré-processar a imagem para criar um blob de imagem que pode ser alimentado para a rede neural)\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=0.00392, size=(416, 416), swapRB=True, crop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passar a imagem pela rede YOLOv3 para detecção de objetos\n",
    "net.setInput(blob) # para obter as detecções\n",
    "outs = net.forward(output_layers) # camadas de saída da rede"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processar as detecções e mostrar na imagem\n",
    "class_ids = []\n",
    "confidences = []\n",
    "boxes = []\n",
    "\n",
    "#Analise as detecções retornadas pelas camadas de saída da rede.\n",
    "#A classe com a maior pontuação é selecionada como a classe identificada para cada detecção.\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "        if confidence > 0.5:  # Defina um limiar de confiança\n",
    "            # Coordenadas da caixa delimitadora\n",
    "            center_x = int(detection[0] * image.shape[1])\n",
    "            center_y = int(detection[1] * image.shape[0])\n",
    "            w = int(detection[2] * image.shape[1])\n",
    "            h = int(detection[3] * image.shape[0])\n",
    "\n",
    "            # Coordenadas do canto superior esquerdo\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            boxes.append([x, y, w, h])\n",
    "            confidences.append(float(confidence))\n",
    "            class_ids.append(class_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para eliminar detecções redundantes\n",
    "indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) # aplicando a supressão não máxima às caixas delimitadoras\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "# Utilizando as informações das detecções para desenhar caixas delimitadoras e exibir o nome da classe e a confiança associada.\n",
    "for i in range(len(boxes)):\n",
    "    if i in indexes:\n",
    "        x, y, w, h = boxes[i]\n",
    "        label = str(classes[class_ids[i]])\n",
    "        confidence = confidences[i]\n",
    "        color = (255, 0, 0)\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, f\"{label}\", (x, y - 5), font, 1, color, 1)\n",
    "        #cv2.putText(image, f\"{label} {confidence:.2f}\", (x, y - 5), font, 1, color, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar a imagem resultante com as detecções desenhadas\n",
    "cv2.imshow(\"Deteccao de Residuos\", image)\n",
    "cv2.imshow(\"Imagem original\", image_original)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
